# PageIndex + vLLM Integration

> **Intelligent Document Analysis & Chat Interface powered by Qwen3-32B reasoning on dual RTX 5090 GPUs**

[![Python](https://img.shields.io/badge/Python-3.11-blue.svg)](https://www.python.org/)
[![Flask](https://img.shields.io/badge/Flask-3.1.0-green.svg)](https://flask.palletsprojects.com/)
[![Docker](https://img.shields.io/badge/Docker-Compose-2496ED.svg)](https://docs.docker.com/compose/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![vLLM](https://img.shields.io/badge/vLLM-Latest-purple.svg)](https://docs.vllm.ai/)

A production-ready web application that combines **PageIndex**'s intelligent document structure extraction with **vLLM**'s high-performance inference engine, running **Qwen3-32B-AWQ** reasoning model on dual NVIDIA RTX 5090 GPUs.

![PageIndex Demo](docs/images/demo.png)
*Modern dark-themed UI with document management and AI-powered chat*

---

## ğŸŒŸ Features

### ğŸ“„ Document Intelligence
- **AI-Powered Structure Extraction**: Automatically analyze PDFs and Markdown files
- **Hierarchical Analysis**: Extract table of contents, sections, and summaries
- **Multi-Format Support**: PDF, Markdown (.md, .markdown)
- **Batch Processing**: Upload and process multiple documents
- **Smart Chunking**: Intelligent page/token-based segmentation

### ğŸ’¬ Intelligent Chat
- **Context-Aware Responses**: Chat with AI using document context
- **Reasoning Transparency**: View the model's thinking process
- **Persistent History**: All conversations saved and retrievable
- **Real-time Streaming**: Live typing indicators and smooth UX
- **Multi-Document**: Switch between document contexts seamlessly

### ğŸš€ Performance
- **Dual GPU Acceleration**: Tensor parallelism across 2x RTX 5090
- **High Throughput**: vLLM optimized inference (30-50 tokens/sec)
- **Efficient Memory**: AWQ 4-bit quantization (60GB VRAM usage)
- **Fast Loading**: Model cached for instant startup

### ğŸ¨ Modern Web Interface
- **Clean Dark Theme**: Professional, eye-friendly design
- **Responsive Layout**: Desktop and tablet optimized
- **Real-time Updates**: Live document processing status
- **Toast Notifications**: Non-intrusive user feedback
- **Keyboard Shortcuts**: Power user friendly

---

## ğŸ“‹ Table of Contents

- [System Requirements](#-system-requirements)
- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Usage](#-usage)
- [Architecture](#-architecture)
- [API Documentation](#-api-documentation)
- [Configuration](#-configuration)
- [Performance](#-performance)
- [Troubleshooting](#-troubleshooting)
- [Contributing](#-contributing)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)

---

## ğŸ’» System Requirements

### Hardware
- **GPU**: 2x NVIDIA RTX 5090 (or 2x A100 80GB, H100)
  - Minimum: 1x RTX 4090 (24GB VRAM) - single GPU mode
- **RAM**: 32GB+ system memory
- **Storage**: 50GB+ for model cache and data
- **CPU**: 8+ cores recommended

### Software
- **OS**: Ubuntu 20.04+ / Linux with CUDA support
- **Docker**: 24.0+ with Docker Compose v2
- **NVIDIA Driver**: 535+ (CUDA 12.1+)
- **NVIDIA Container Toolkit**: Latest

---

## ğŸš€ Quick Start

### One-Command Deploy

```bash
# Clone the repository
git clone https://github.com/yourusername/pageindex-vllm.git
cd pageindex-vllm

# Start all services
docker compose -f config/docker-compose.yml up -d

# Wait for model loading (~3-5 minutes first time)
# Watch progress
docker logs pageindex-vllm -f

# Access web interface
open http://localhost:8090
```

**That's it!** The system will:
1. Pull vLLM and build PageIndex containers
2. Download Qwen3-32B-AWQ model (~17GB)
3. Load model across both GPUs
4. Start the web interface

---

## ğŸ“¦ Installation

### Step 1: Prerequisites

```bash
# Install NVIDIA drivers (if not already installed)
ubuntu-drivers devices
sudo ubuntu-drivers autoinstall

# Install NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list

sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo systemctl restart docker
```

### Step 2: Clone & Configure

```bash
# Clone repository
git clone https://github.com/yourusername/pageindex-vllm.git
cd pageindex-vllm

# Review configuration
cat config/.env

# Optional: Customize settings
nano config/.env
```

### Step 3: Deploy

```bash
# Build and start services
docker compose -f config/docker-compose.yml up -d

# Monitor deployment
docker compose -f config/docker-compose.yml logs -f
```

### Step 4: Verify

```bash
# Check container health
docker ps

# Test vLLM API
curl http://localhost:8000/v1/models

# Test web interface
curl http://localhost:8090/health

# Open browser
open http://localhost:8090
```

---

## ğŸ“– Usage

### Web Interface

#### 1. Upload a Document

```
1. Open http://localhost:8090
2. Click "Upload Document"
3. Select PDF or Markdown file
4. Watch auto-processing (status updates in real-time)
```

#### 2. Chat Without Context

```
1. Type your question in the chat input
2. Press Enter or click Send
3. View response with reasoning process
4. Click "ğŸ’­ Reasoning Process" to see AI's thinking
```

#### 3. Chat With Document Context

```
1. Wait for document processing to complete
2. Click on a processed document in the sidebar
3. Context indicator appears at top
4. Ask questions about the document
5. AI uses document structure for better answers
```

### Command Line Interface

#### Process a Document

```bash
# Process PDF
docker exec pageindex-app python run_pageindex.py \
  --pdf_path /app/data/document.pdf \
  --model Qwen/Qwen3-32B-AWQ \
  --if-add-node-summary yes

# Process Markdown
docker exec pageindex-app python run_pageindex.py \
  --md_path /app/data/readme.md \
  --model Qwen/Qwen3-32B-AWQ \
  --if-thinning yes
```

#### Direct API Usage

```python
import requests

# Chat API
response = requests.post('http://localhost:8090/api/chat', json={
    'message': 'Explain quantum computing',
    'document': 'quantum_paper.pdf'  # Optional
})
print(response.json()['response'])

# vLLM Direct API (OpenAI compatible)
response = requests.post('http://localhost:8000/v1/chat/completions', json={
    'model': 'Qwen/Qwen3-32B-AWQ',
    'messages': [{'role': 'user', 'content': 'Hello!'}]
})
print(response.json())
```

---

## ğŸ—ï¸ Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Web Browser                             â”‚
â”‚                  (localhost:8090)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP/REST API
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              PageIndex Container                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Flask Web Application                             â”‚    â”‚
â”‚  â”‚  â”œâ”€ Document Upload & Management                   â”‚    â”‚
â”‚  â”‚  â”œâ”€ PageIndex Processing Engine                    â”‚    â”‚
â”‚  â”‚  â”œâ”€ Chat API with Context Injection                â”‚    â”‚
â”‚  â”‚  â””â”€ History Persistence (JSON)                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  PageIndex Library                                 â”‚    â”‚
â”‚  â”‚  â”œâ”€ PDF Parser (PyMuPDF)                           â”‚    â”‚
â”‚  â”‚  â”œâ”€ Markdown Parser                                â”‚    â”‚
â”‚  â”‚  â”œâ”€ Structure Extractor                            â”‚    â”‚
â”‚  â”‚  â””â”€ LLM Integration                                â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ OpenAI-compatible API
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                vLLM Container (Port 8000)                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  vLLM Inference Engine                             â”‚    â”‚
â”‚  â”‚  â”œâ”€ Model: Qwen3-32B-AWQ                           â”‚    â”‚
â”‚  â”‚  â”œâ”€ Quantization: 4-bit AWQ                        â”‚    â”‚
â”‚  â”‚  â”œâ”€ Tensor Parallelism: TP=2                       â”‚    â”‚
â”‚  â”‚  â”œâ”€ NCCL 2.27.7 Multi-GPU Communication            â”‚    â”‚
â”‚  â”‚  â””â”€ OpenAI API Compatibility                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                         â”‚                                   â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚              â–¼                     â–¼                        â”‚
â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚       â”‚  GPU 0      â”‚       â”‚  GPU 1      â”‚                â”‚
â”‚       â”‚ RTX 5090    â”‚       â”‚ RTX 5090    â”‚                â”‚
â”‚       â”‚ 31GB/32GB   â”‚       â”‚ 29GB/32GB   â”‚                â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

| Component | Technology | Purpose |
|-----------|-----------|---------|
| **Frontend** | HTML5, CSS3, Vanilla JS | Modern web interface |
| **Backend** | Python 3.11, Flask 3.1 | REST API server |
| **Document Processing** | PageIndex, PyMuPDF, PyPDF2 | PDF/MD parsing |
| **LLM Inference** | vLLM 0.6+ | High-performance serving |
| **Model** | Qwen3-32B-AWQ | Reasoning & generation |
| **Quantization** | AWQ 4-bit | Memory efficiency |
| **Multi-GPU** | Tensor Parallelism (TP=2) | Dual GPU distribution |
| **Communication** | NCCL 2.27.7 | GPU synchronization |
| **Orchestration** | Docker Compose | Service management |
| **Storage** | JSON files | Data persistence |

---

## ğŸ“¡ API Documentation

### Web API Endpoints

#### Document Management

**List Documents**
```http
GET /api/documents
```

**Upload Document**
```http
POST /api/upload
Content-Type: multipart/form-data

file: <binary>
```

**Process Document**
```http
POST /api/process/<filename>
```

**Delete Document**
```http
DELETE /api/documents/<filename>
```

**Get Document Structure**
```http
GET /api/document/<filename>/structure
```

#### Chat

**Send Message**
```http
POST /api/chat
Content-Type: application/json

{
  "message": "Your question here",
  "document": "optional_document.pdf"
}
```

**Get Chat History**
```http
GET /api/chat/history
```

**Clear Chat History**
```http
DELETE /api/chat/history
```

#### System

**Health Check**
```http
GET /health

Response:
{
  "status": "healthy",
  "model": "Qwen/Qwen3-32B-AWQ"
}
```

### vLLM API (OpenAI Compatible)

**Chat Completions**
```http
POST http://localhost:8000/v1/chat/completions
Content-Type: application/json

{
  "model": "Qwen/Qwen3-32B-AWQ",
  "messages": [
    {"role": "user", "content": "Hello!"}
  ],
  "temperature": 0.7,
  "max_tokens": 2048
}
```

**List Models**
```http
GET http://localhost:8000/v1/models
```

---

## âš™ï¸ Configuration

### Environment Variables

**vLLM Configuration** (`config/.env`)
```bash
# Model Selection
VLLM_MODEL=Qwen/Qwen3-32B-AWQ

# GPU Settings
VLLM_TENSOR_PARALLEL_SIZE=2          # Number of GPUs
VLLM_GPU_MEMORY_UTILIZATION=0.80     # 80% VRAM usage

# Performance Tuning
VLLM_MAX_MODEL_LEN=32768             # Max context length
VLLM_MAX_NUM_SEQS=256                # Batch size
VLLM_QUANTIZATION=awq                # Quantization method

# NCCL (Multi-GPU Communication)
NCCL_VERSION=2.27.7-1+cuda12.9
NCCL_P2P_DISABLE=0                   # Enable peer-to-peer
NCCL_IB_DISABLE=1                    # Disable InfiniBand
```

**PageIndex Configuration** (`pageindex-src/.env`)
```bash
# vLLM Integration
VLLM_BASE_URL=http://vllm:8000/v1
CHATGPT_API_KEY=not-needed

# Model
DEFAULT_MODEL=Qwen/Qwen3-32B-AWQ
```

### Docker Compose Configuration

**Volumes** (for data persistence)
```yaml
services:
  pageindex:
    volumes:
      - ./data/uploads:/app/data/uploads
      - ./data/results:/app/data/results
      - ./data/chat_history.json:/app/data/chat_history.json
```

**Resource Limits**
```yaml
services:
  vllm:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2  # Number of GPUs
              capabilities: [gpu]
```

### PageIndex Processing Options

```python
# PDF Processing
config(
    model='Qwen/Qwen3-32B-AWQ',
    toc_check_page_num=20,           # Pages to scan for ToC
    max_page_num_each_node=10,       # Pages per chunk
    max_token_num_each_node=20000,   # Tokens per chunk
    if_add_node_summary='yes',       # Generate summaries
    if_add_doc_description='no',     # Document overview
    if_add_node_text='no'            # Include full text
)
```

---

## ğŸ“Š Performance

### Benchmarks

**Model Loading**
- First-time download: ~3-5 minutes (17GB model)
- Cached loading: ~30 seconds
- GPU memory allocation: ~5 seconds

**Document Processing**
| Document Size | Processing Time | Complexity |
|--------------|----------------|------------|
| 10 pages PDF | 30-45 seconds | Simple |
| 50 pages PDF | 1-3 minutes | Moderate |
| 100 pages PDF | 3-5 minutes | Complex |
| Markdown (<100KB) | 10-30 seconds | Variable |

**Chat Performance**
- Latency (first token): ~200-500ms
- Throughput: 30-50 tokens/second
- Context length: Up to 32,768 tokens
- Batch size: Up to 256 concurrent requests

**GPU Utilization**
```
GPU 0: 31.0GB / 32.6GB (95%)
GPU 1: 29.5GB / 32.6GB (90%)
Total: 60.5GB / 65.2GB (93%)
```

### Optimization Tips

**For Faster Processing**
```bash
# Increase batch size
VLLM_MAX_NUM_SEQS=512

# Reduce context length if not needed
VLLM_MAX_MODEL_LEN=16384

# Increase GPU memory usage
VLLM_GPU_MEMORY_UTILIZATION=0.90
```

**For Lower Memory Usage**
```bash
# Single GPU mode
VLLM_TENSOR_PARALLEL_SIZE=1

# Reduce batch size
VLLM_MAX_NUM_SEQS=128

# Shorter context
VLLM_MAX_MODEL_LEN=8192
```

---

## ğŸ› Troubleshooting

### Common Issues

#### Container Won't Start

```bash
# Check NVIDIA runtime
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi

# Verify Docker Compose version
docker compose version  # Should be v2.x

# Check logs
docker compose -f config/docker-compose.yml logs
```

#### Out of Memory

```bash
# Reduce GPU memory utilization
VLLM_GPU_MEMORY_UTILIZATION=0.70

# Or use single GPU
VLLM_TENSOR_PARALLEL_SIZE=1
```

#### Slow Processing

```bash
# Check GPU usage
nvidia-smi -l 1

# Increase workers
VLLM_MAX_NUM_SEQS=512

# Monitor vLLM
docker logs pageindex-vllm -f
```

#### Port Already in Use

```bash
# Change port in docker-compose.yml
ports:
  - "8091:8080"  # Web UI
  - "8001:8000"  # vLLM
```

### Debug Commands

```bash
# View all logs
docker compose -f config/docker-compose.yml logs -f

# Shell into container
docker exec -it pageindex-app bash

# Check model files
docker exec pageindex-vllm ls -lh /root/.cache/huggingface

# Test vLLM directly
curl http://localhost:8000/v1/models

# Monitor GPU
watch -n 1 nvidia-smi
```

---

## ğŸ¤ Contributing

We welcome contributions! Please follow these guidelines:

### Development Setup

```bash
# Fork and clone
git clone https://github.com/yourusername/pageindex-vllm.git
cd pageindex-vllm

# Create branch
git checkout -b feature/your-feature

# Install pre-commit hooks
pip install pre-commit
pre-commit install

# Make changes and test
docker compose -f config/docker-compose.yml up --build

# Submit PR
git push origin feature/your-feature
```

### Code Style

- **Python**: PEP 8, Black formatting
- **JavaScript**: ESLint, Prettier
- **HTML/CSS**: Semantic markup, BEM methodology

### Testing

```bash
# Run tests (when implemented)
docker exec pageindex-app pytest

# Lint
docker exec pageindex-app flake8 webapp/
docker exec pageindex-app eslint webapp/static/js/
```

### Areas for Contribution

- [ ] Unit tests for backend
- [ ] E2E tests for web UI
- [ ] Additional document formats (DOCX, EPUB)
- [ ] User authentication
- [ ] Real-time collaboration
- [ ] Mobile app
- [ ] Performance optimizations
- [ ] Documentation improvements

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

### Core Technologies

- **[PageIndex](https://github.com/edwardzjl/PageIndex)** - Intelligent document structure extraction
- **[vLLM](https://docs.vllm.ai/)** - High-performance LLM inference engine
- **[Qwen](https://github.com/QwenLM/Qwen)** - State-of-the-art language model by Alibaba Cloud
- **[Flask](https://flask.palletsprojects.com/)** - Web framework
- **[Docker](https://www.docker.com/)** - Containerization platform

### Dependencies

- **PyMuPDF** - PDF processing
- **PyPDF2** - PDF utilities
- **tiktoken** - Token counting
- **OpenAI Python** - API client library
- **python-dotenv** - Environment management

### Inspiration

This project combines the best of:
- Document intelligence (PageIndex)
- High-performance inference (vLLM)
- Reasoning capabilities (Qwen3)
- Modern web design principles

---

## ğŸ“ Support

### Documentation

- [Web UI Guide](WEB_UI_GUIDE.md) - Complete web interface documentation
- [CLI Usage](USAGE.md) - Command-line interface guide
- [API Reference](docs/API.md) - Detailed API documentation

### Getting Help

- **Issues**: [GitHub Issues](https://github.com/yourusername/pageindex-vllm/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/pageindex-vllm/discussions)
- **Wiki**: [Project Wiki](https://github.com/yourusername/pageindex-vllm/wiki)

### Community

- **Discord**: [Join our server](https://discord.gg/yourserver)
- **Twitter**: [@pageindex](https://twitter.com/pageindex)

---

## ğŸ—ºï¸ Roadmap

### v1.0 (Current)
- [x] Basic web UI
- [x] Document upload & processing
- [x] Chat with context
- [x] Dual GPU support
- [x] Chat history

### v1.1 (Planned)
- [ ] Real-time progress updates
- [ ] Document annotations
- [ ] Export to various formats
- [ ] Advanced search
- [ ] User authentication

### v2.0 (Future)
- [ ] Multi-user support
- [ ] Team collaboration
- [ ] API rate limiting
- [ ] Cloud deployment guides
- [ ] Mobile application
- [ ] Plugin system

---

## ğŸ“ˆ Project Stats

![GitHub stars](https://img.shields.io/github/stars/yourusername/pageindex-vllm?style=social)
![GitHub forks](https://img.shields.io/github/forks/yourusername/pageindex-vllm?style=social)
![GitHub watchers](https://img.shields.io/github/watchers/yourusername/pageindex-vllm?style=social)

![GitHub issues](https://img.shields.io/github/issues/yourusername/pageindex-vllm)
![GitHub pull requests](https://img.shields.io/github/issues-pr/yourusername/pageindex-vllm)
![GitHub last commit](https://img.shields.io/github/last-commit/yourusername/pageindex-vllm)

---

## â­ Star History

[![Star History Chart](https://api.star-history.com/svg?repos=yourusername/pageindex-vllm&type=Date)](https://star-history.com/#yourusername/pageindex-vllm&Date)

---

<div align="center">

**Built with â¤ï¸ using PageIndex, vLLM, and Qwen3**

[â¬† Back to Top](#pageindex--vllm-integration)

</div>
